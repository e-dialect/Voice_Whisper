import os
import threading
import queue
import uuid
import time
from datetime import timedelta, datetime
from pydub import AudioSegment
import ffmpeg
from funasr import AutoModel
from flask import Flask, render_template, request, jsonify, send_from_directory
import werkzeug.utils
import re
import sys

sys.path.append('/home/chihan/workspace/SenseVoice/tools')
sys.path.append('/home/chihan/workspace/SenseVoice/tools/uvr5')

# æ·»åŠ CosyVoiceè·¯å¾„
sys.path.append('/home/chihan/workspace/SenseVoice/CosyVoice')
sys.path.append('/home/chihan/workspace/SenseVoice/CosyVoice/third_party/Matcha-TTS')

# å¯¼å…¥CosyVoice
try:
    from cosyvoice.cli.cosyvoice import CosyVoice2
    from cosyvoice.utils.file_utils import load_wav
    import torchaudio
    cosyvoice_available = True
except ImportError:
    print("è­¦å‘Š: CosyVoiceæ¨¡å—æœªå®‰è£…ï¼Œå£°éŸ³å…‹éš†åŠŸèƒ½å°†ä¸å¯ç”¨")
    cosyvoice_available = False

# åœ¨å…¨å±€å˜é‡éƒ¨åˆ†æ·»åŠ 
cosyvoice_model = None
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['OUTPUT_FOLDER'] = 'outputs'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['OUTPUT_FOLDER'], exist_ok=True)

# å…¨å±€é˜Ÿåˆ—
spk_txt_queue = queue.Queue()
result_queue = queue.Queue()
audio_concat_queue = queue.Queue()

# æ”¯æŒçš„éŸ³è§†é¢‘æ ¼å¼
support_audio_format = ['.mp3', '.m4a', '.aac', '.ogg', '.wav', '.flac', '.wma', '.aif']
support_video_format = ['.mp4', '.avi', '.mov', '.mkv']

# æ¨¡å‹é…ç½®éƒ¨åˆ†
home_directory = os.path.expanduser("~")

# ASRæ¨¡å‹é…ç½®
models = {
    "timestamp": {  # æ·»åŠ æ—¶é—´æˆ³ä¸“ç”¨æ¨¡å‹
        "path": "/home/chihan/workspace/SenseVoice/model_cache/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
        "revision": "None",  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision
        "name": "æ—¶é—´æˆ³æ¨¡å‹(ä¸å¯é€‰)"
    },
    "sensevoice": {
        "path": "/home/chihan/workspace/SenseVoice/model_cache/models/iic/SenseVoiceSmall",
        "revision": "None",  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision
        "name": "SenseVoiceæ ‡å‡†æ™®é€šè¯"
    },
    "jiang_huai": {
        "path": "/home/chihan/workspace/SenseVoice/outputs/Jiang-Huai",
        "revision": None,  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision
        "name": "æ±Ÿæ·®æ–¹è¨€"
    },
    "northeastern": {
        "path": "/home/chihan/workspace/SenseVoice/outputs/northeastern",
        "revision": None,  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision
        "name": "ä¸œåŒ—æ–¹è¨€"
    },
    "southwestern": {
        "path": "/home/chihan/workspace/SenseVoice/outputs/south_western",
        "revision": None,  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision
        "name": "è¥¿å—æ–¹è¨€"
    }
}

# é»˜è®¤ä½¿ç”¨SenseVoiceæ¨¡å‹
default_model = "sensevoice"

# ä¿ç•™å…¶ä»–å¿…è¦çš„é¢„è®­ç»ƒæ¨¡å‹ - ç›´æ¥ä½¿ç”¨ModelScopeæ ¼å¼çš„è·¯å¾„
vad_model_path = "/home/chihan/workspace/SenseVoice/model_cache/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch"
vad_model_revision = "None"  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision
punc_model_path = "/home/chihan/workspace/SenseVoice/model_cache/models/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch"
punc_model_revision = "None"  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision
spk_model_path = "/home/chihan/workspace/SenseVoice/model_cache/models/iic/speech_campplus_sv_zh-cn_16k-common"
spk_model_revision = "None"  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision

# æ ¹æ®ç³»ç»Ÿé…ç½®é€‰æ‹©è®¾å¤‡
try:
    import torch
    ngpu = 1 if torch.cuda.is_available() else 0
    device = "cuda:7" if torch.cuda.is_available() else "cpu"
except:
    ngpu = 0
    device = "cpu"
    
ncpu = os.cpu_count() or 4



# é™å™ªç›¸å…³æ¨¡å—å¯¼å…¥
try:
    from tools.my_utils import load_audio, clean_path
    # æ­£ç¡®å¯¼å…¥UVR5æ¨¡å—ä¸­çš„ç±»ï¼Œè€Œä¸æ˜¯å‡½æ•°
    from tools.uvr5.vr import AudioPre, AudioPreDeEcho
    from tools.uvr5.mdxnet import MDXNetDereverb
    uvr_available = True
    print("é™å™ªæ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"è­¦å‘Š: é™å™ªæ¨¡å—åŠ è½½å¤±è´¥: {e}")
    uvr_available = False
# åœ¨æ¨¡å‹é…ç½®éƒ¨åˆ†æ·»åŠ uvr5æ¨¡å‹é…ç½®
uvr5_models = {
    "HP2": "/home/chihan/workspace/SenseVoice/tools/uvr5/uvr5_weights/HP2_all_vocals.pth",
    "HP5": "/home/chihan/workspace/SenseVoice/tools/uvr5/uvr5_weights/HP5_only_main_vocal.pth",
    "VR-DeEcho": "/home/chihan/workspace/SenseVoice/tools/uvr5/uvr5_weights/VR-DeEchoNormal.pth",
    "VR-DeEcho-Aggressive": "/home/chihan/workspace/SenseVoice/tools/uvr5/uvr5_weights/VR-DeEchoAggressive.pth",
    "VR-DeEcho-DeReverb": "/home/chihan/workspace/SenseVoice/tools/uvr5/uvr5_weights/VR-DeEchoDeReverb.pth",
    "onnx_dereverb_By_FoxJoy": "/home/chihan/workspace/SenseVoice/tools/uvr5/uvr5_weights/onnx_dereverb_By_FoxJoy/vocals.onnx",
}

# ä¿®æ”¹process_denoiseå‡½æ•°ä»¥åŒ¹é…webui.pyçš„uvrå‡½æ•°å®ç°
def process_denoise(input_file, model_name="HP5", agg_level=10):
    """
    å¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œé™å™ªå¤„ç†
    
    å‚æ•°:
    input_file: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    model_name: é™å™ªæ¨¡å‹åç§°
    agg_level: é™å™ªæ¿€è¿›ç¨‹åº¦ (0-20)
    
    è¿”å›:
    æˆåŠŸæ—¶è¿”å›å¤„ç†åçš„éŸ³é¢‘è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    if not uvr_available:
        print("é™å™ªæ¨¡å—ä¸å¯ç”¨")
        return None
    
    # æ”¹è¿›é‡å¤å¤„ç†æ£€æµ‹ - æ›´ç²¾ç¡®çš„åˆ¤æ–­
    if 'denoised' in input_file or 'vocal_' in input_file:
        print(f"æ–‡ä»¶å·²ç»è¿‡é™å™ªå¤„ç†ï¼Œè·³è¿‡: {input_file}")
        return input_file
        
    try:
        # ç¡®ä¿è¾“å…¥æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(input_file):
            print(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return None

        # ç¡®ä¿æ¨¡å‹å­˜åœ¨
        if model_name not in uvr5_models:
            print(f"æ‰¾ä¸åˆ°æŒ‡å®šçš„é™å™ªæ¨¡å‹: {model_name}")
            model_name = "HP5"  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_vocal_dir = os.path.join(app.config['OUTPUT_FOLDER'], 'denoised', 'vocal')
        output_inst_dir = os.path.join(app.config['OUTPUT_FOLDER'], 'denoised', 'inst')
        os.makedirs(output_vocal_dir, exist_ok=True)
        os.makedirs(output_inst_dir, exist_ok=True)
        
        # æå‰å®šä¹‰ç¡®å®šçš„è¾“å‡ºæ–‡ä»¶åï¼Œé¿å…åç»­çŒœæµ‹
        timestamp = int(time.time())
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        # é¿å…é•¿æ–‡ä»¶å
        if len(base_name) > 20:
            base_name = base_name[:20]
            
        # é¢„å…ˆç¡®å®šè¾“å‡ºæ–‡ä»¶å - ä¸ä½¿ç”¨UVR5çš„è‡ªåŠ¨å‘½å
        output_filename = f"denoised_{base_name}_{timestamp}.wav"
        output_vocal_path = os.path.join(output_vocal_dir, output_filename)
        
        # é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹å¤„ç†ç±»
        is_hp3 = "HP3" in model_name
        device = "cuda" if torch.cuda.is_available() else "cpu"
        is_half = torch.cuda.is_available() # å¦‚æœç”¨GPUåˆ™ä½¿ç”¨åŠç²¾åº¦
        
        # éŸ³é¢‘æ ¼å¼è½¬æ¢
        need_reformat = True
        inp_path = input_file
        tmp_path = None
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ ¼å¼åŒ–
            info = ffmpeg.probe(inp_path, cmd="ffprobe")
            if (
                info["streams"][0]["channels"] == 2
                and info["streams"][0]["sample_rate"] == "44100"
            ):
                need_reformat = False
        except Exception as e:
            print(f"æ£€æŸ¥éŸ³é¢‘æ ¼å¼å¤±è´¥: {e}")
            need_reformat = True
            
        # å¦‚æœéœ€è¦é‡æ–°æ ¼å¼åŒ–ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        if need_reformat:
            print("éŸ³é¢‘éœ€è¦é‡æ–°æ ¼å¼åŒ–ä¸ºæ ‡å‡†æ ¼å¼")
            tmp_path = os.path.join(app.config['UPLOAD_FOLDER'], f"temp_{base_name}_{timestamp}.wav")
            try:
                os.system(
                    f'ffmpeg -i "{inp_path}" -vn -acodec pcm_s16le -ac 2 -ar 44100 "{tmp_path}" -y'
                )
                inp_path = tmp_path
                print(f"æ ¼å¼åŒ–éŸ³é¢‘å®Œæˆ: {tmp_path}")
            except Exception as format_err:
                print(f"æ ¼å¼åŒ–éŸ³é¢‘å¤±è´¥: {format_err}")
                return None
        
        # åˆ›å»ºå¤„ç†å™¨å¹¶æ‰§è¡ŒéŸ³é¢‘å¤„ç†
        try:
            if model_name == "onnx_dereverb_By_FoxJoy":
                pre_fun = MDXNetDereverb(15)
            else:
                func = AudioPre if "DeEcho" not in model_name else AudioPreDeEcho
                pre_fun = func(
                    agg=int(agg_level),
                    model_path=uvr5_models[model_name],
                    device=device,
                    is_half=is_half,
                )
            
            # æ‰§è¡ŒéŸ³é¢‘å¤„ç† - ä½¿ç”¨UVR5çš„å¤„ç†
            format0 = "wav"
            pre_fun._path_audio_(inp_path, output_inst_dir, output_vocal_dir, format0, is_hp3)
            
            # æ¸…ç†èµ„æº
            try:
                if model_name == "onnx_dereverb_By_FoxJoy":
                    del pre_fun.pred.model
                    del pre_fun.pred.model_
                else:
                    del pre_fun.model
                    del pre_fun
            except Exception as clean_err:
                print(f"æ¸…ç†æ¨¡å‹èµ„æºæ—¶å‡ºé”™: {clean_err}")
                
            # é‡Šæ”¾CUDAå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æŸ¥æ‰¾UVR5å¤„ç†åçš„æœ€æ–°æ–‡ä»¶ - ç”¨ç»å¯¹æ—¶é—´è€Œä¸æ˜¯æ–‡ä»¶å
            newest_file = None
            newest_time = 0
            
            for file in os.listdir(output_vocal_dir):
                file_path = os.path.join(output_vocal_dir, file)
                # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                file_time = os.path.getmtime(file_path)
                
                # é€‰æ‹©æœ€æ–°åˆ›å»ºçš„æ–‡ä»¶
                if file_time > newest_time:
                    newest_time = file_time
                    newest_file = file_path
            
            # å¦‚æœæ‰¾åˆ°æ–‡ä»¶ï¼Œå°†å…¶é‡å‘½åä¸ºæˆ‘ä»¬é¢„å®šä¹‰çš„æ–‡ä»¶å
            if newest_file and os.path.exists(newest_file):
                try:
                    # ä½¿ç”¨é¢„å®šä¹‰çš„åç§°ï¼Œç¡®ä¿ä¸€è‡´æ€§
                    os.rename(newest_file, output_vocal_path)
                    print(f"å°†é™å™ªè¾“å‡ºæ–‡ä»¶é‡å‘½åä¸º: {output_vocal_path}")
                    vocal_file = output_vocal_path
                except Exception as rename_err:
                    print(f"é‡å‘½åå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶: {rename_err}")
                    vocal_file = newest_file
            else:
                print("æ‰¾ä¸åˆ°é™å™ªå¤„ç†è¾“å‡ºæ–‡ä»¶")
                return None
                
            # è¿”å›å¤„ç†åçš„æ–‡ä»¶è·¯å¾„
            if vocal_file and os.path.exists(vocal_file):
                print(f"é™å™ªæˆåŠŸ: {vocal_file}")
                return vocal_file
            else:
                print(f"æ‰¾ä¸åˆ°è¾“å‡ºçš„äººå£°æ–‡ä»¶")
                return None
                
        except Exception as process_err:
            print(f"éŸ³é¢‘å¤„ç†å¼‚å¸¸: {process_err}")
            import traceback
            traceback.print_exc()
            return None
            
    except Exception as e:
        print(f"é™å™ªå¤„ç†å¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if 'tmp_path' in locals() and tmp_path and os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
                print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {tmp_path}")
            except:
                pass
# é™å™ªAPIç«¯ç‚¹
@app.route('/api/denoise', methods=['POST'])
def denoise_audio():
    """é™å™ªAPI"""
    if not uvr_available:
        return jsonify({"status": "error", "message": "é™å™ªæ¨¡å—ä¸å¯ç”¨"})
        
    try:
        if 'file' not in request.files:
            return jsonify({"status": "error", "message": "æœªé€‰æ‹©æ–‡ä»¶"})
            
        file = request.files['file']
        if file.filename == '':
            return jsonify({"status": "error", "message": "æœªé€‰æ‹©æ–‡ä»¶"})
            
        # è·å–å‚æ•°
        model_name = request.form.get('model', "HP5")
        try:
            agg_level = int(request.form.get('agg_level', 10))
        except ValueError:
            agg_level = 10  # é»˜è®¤å€¼
            
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        filename = werkzeug.utils.secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"denoise_{int(time.time())}_{filename}")
        file.save(file_path)
            
        # æ‰§è¡Œé™å™ªå¤„ç†
        denoised_file = process_denoise(file_path, model_name, agg_level)
        
        # æ£€æŸ¥å¤„ç†ç»“æœ
        if denoised_file:
            # ç”Ÿæˆç›¸å¯¹URLè·¯å¾„
            relative_path = os.path.relpath(denoised_file, app.config['OUTPUT_FOLDER'])
            audio_url = f"/outputs/{relative_path}"
            
            return jsonify({
                "status": "success",
                "message": "é™å™ªå¤„ç†æˆåŠŸ",
                "original_file": file_path,
                "denoised_file": denoised_file,
                "audio_url": audio_url
            })
        else:
            return jsonify({
                "status": "error",
                "message": "é™å™ªå¤„ç†å¤±è´¥"
            })
            
    except Exception as e:
        print(f"é™å™ªAPIå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error", 
            "message": f"é™å™ªå¤„ç†å¤±è´¥: {str(e)}"
        })
# æ¨¡å‹å­—å…¸ï¼Œç”¨äºç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹
loaded_models = {}
# æ·»åŠ åˆ°è¾…åŠ©å‡½æ•°éƒ¨åˆ†
def clean_emoji_text(text):
    """ç§»é™¤æ–‡æœ¬ä¸­çš„æ‰€æœ‰è¡¨æƒ…ç¬¦å·"""
    if not text:
        return ""
        
    # ç§»é™¤æ‰€æœ‰emo_setå’Œevent_setä¸­çš„è¡¨æƒ…ç¬¦å·
    clean_text = text
    for emoji in list(emo_set) + list(event_set):
        clean_text = clean_text.replace(emoji, "")
        
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    return clean_text
# æ·»åŠ æƒ…æ„Ÿè¯†åˆ«æ ¼å¼åŒ–å‡½æ•°
def format_emotion_text(text):
    """å°†SenseVoiceæƒ…æ„Ÿè¯†åˆ«ç»“æœæ ¼å¼åŒ–ä¸ºè¡¨æƒ…å’Œçº¯æ–‡æœ¬"""
    if text is None:
        print("è­¦å‘Š: format_emotion_textæ”¶åˆ°Noneè¾“å…¥")
        return {
            "text": "",
            "emotion": "ğŸ˜",
            "events": "",
            "formatted_text": ""
        }
        
    # ä¿å­˜åŸå§‹æ–‡æœ¬ä»¥ä¾¿è°ƒè¯•
    original = text
    clean_text = text
    
    try:
        # ç§»é™¤æ‰€æœ‰ç‰¹æ®Šæ ‡è®°å¹¶æ›¿æ¢ä¸ºè¡¨æƒ…
        for tag, emoji in emoji_dict.items():
            if tag in clean_text:
                clean_text = clean_text.replace(tag, emoji)
        
        # æ”¶é›†æ‰€æœ‰æƒ…æ„Ÿï¼ˆä¸æ˜¯åªä¿ç•™ä¸€ä¸ªï¼‰
        found_emotions = []
        for emoji in emo_set:
            if emoji in clean_text:
                found_emotions.append(emoji)
                clean_text = clean_text.replace(emoji, "")
        
        # å®šä¹‰æƒ…æ„Ÿä¼˜å…ˆçº§ (ä»é«˜åˆ°ä½)
        emotion_priority = ["ğŸ˜¡", "ğŸ˜®", "ğŸ˜°", "ğŸ˜”", "ğŸ˜Š", "ğŸ¤¢", "ğŸ˜"]
        
        # æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©ä¸»è¦æƒ…æ„Ÿ
        main_emotion = "ğŸ˜"  # é»˜è®¤ä¸­æ€§
        if found_emotions:
            # æ‰¾å‡ºä¼˜å…ˆçº§æœ€é«˜çš„æƒ…æ„Ÿ
            for priority_emotion in emotion_priority:
                if priority_emotion in found_emotions:
                    main_emotion = priority_emotion
                    break
        
        # æ”¶é›†æ‰€æœ‰äº‹ä»¶æ ‡è®°
        event_emojis = []
        for emoji in event_set:
            if emoji in clean_text:
                event_emojis.append(emoji)
                clean_text = clean_text.replace(emoji, "")
        
        # æ¸…ç†æ–‡æœ¬å¹¶ç»„åˆ
        result = clean_text.strip()
        prefix = "".join(event_emojis)
        
        print(f"æƒ…æ„Ÿè¯†åˆ«æˆåŠŸ: æƒ…æ„Ÿ={main_emotion}, äº‹ä»¶={prefix}, åŸå§‹æ–‡æœ¬é•¿åº¦={len(original)}")
        if len(found_emotions) > 1:
            print(f"æ£€æµ‹åˆ°å¤šç§æƒ…æ„Ÿ: {found_emotions}, é€‰æ‹© {main_emotion} ä½œä¸ºä¸»è¦æƒ…æ„Ÿ")
        
        return {
            "text": result,
            "emotion": main_emotion,
            "events": prefix,
            "formatted_text": f"{prefix} {result} {main_emotion}".strip(),
            "all_emotions": found_emotions  # æ·»åŠ æ‰€æœ‰æ£€æµ‹åˆ°çš„æƒ…æ„Ÿï¼Œä¾¿äºé«˜çº§åˆ†æ
        }
        
    except Exception as e:
        print(f"æ ¼å¼åŒ–æƒ…æ„Ÿæ–‡æœ¬å‘ç”Ÿé”™è¯¯: {e}")
        return {
            "text": original,
            "emotion": "ğŸ˜",
            "events": "",
            "formatted_text": original
        }
# æ·»åŠ å¯¹å•ä¸ªéŸ³é¢‘ç‰‡æ®µçš„æƒ…æ„Ÿè¯†åˆ«å‡½æ•°
def recognize_emotion(audio_file):
    """ä½¿ç”¨SenseVoiceè¯†åˆ«éŸ³é¢‘çš„æƒ…æ„Ÿ"""
    try:
        # æ£€æŸ¥æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–
        if 'emotion_model' not in globals() or emotion_model is None:
            print("è­¦å‘Š: æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹æœªåŠ è½½")
            return {
                "text": "",
                "emotion": "ğŸ˜",
                "events": "",
                "formatted_text": ""
            }
            
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(audio_file):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
            return {
                "text": "",
                "emotion": "ğŸ˜",
                "events": "",
                "formatted_text": ""
            }
            
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(audio_file)
        if file_size < 100:  # æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ˜¯ç©ºæ–‡ä»¶æˆ–æŸå
            print(f"æ–‡ä»¶å¤ªå°æˆ–ä¸ºç©º: {audio_file}, å¤§å°: {file_size} å­—èŠ‚")
            return {
                "text": "",
                "emotion": "ğŸ˜", 
                "events": "",
                "formatted_text": ""
            }
            
        with open(audio_file, 'rb') as f:
            audio_bytes = f.read()
        
        print(f"æƒ…æ„Ÿè¯†åˆ«: æ­£åœ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶ {audio_file}, å¤§å°: {len(audio_bytes)} å­—èŠ‚")
        
        # è°ƒç”¨æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹
        result = emotion_model.generate(
            input=audio_bytes, 
            cache={},
            language="zh", 
            use_itn=True,
            batch_size_s=60, 
            batch_size=1,
            merge_vad=True,
            emotion=True,           # å¼€å¯æƒ…æ„Ÿè¯†åˆ«
            return_raw_text=True    # è¿”å›å¸¦æ ‡è®°çš„åŸå§‹æ–‡æœ¬
        )
        print(f"æƒ…æ„Ÿè¯†åˆ«ç»“æœ: {result}")        
        # æ·»åŠ å¼ºå¤§çš„æ£€æŸ¥
        if result is None or len(result) == 0:
            print("æƒ…æ„Ÿè¯†åˆ«: æ¨¡å‹è¿”å›ç©ºç»“æœ")
            return {
                "text": "",
                "emotion": "ğŸ˜",
                "events": "",
                "formatted_text": ""
            }
        
        # å°è¯•è·å–æ–‡æœ¬ç»“æœ
        if not isinstance(result[0], dict) or "text" not in result[0]:
            print(f"æƒ…æ„Ÿè¯†åˆ«: æ„å¤–çš„ç»“æœæ ¼å¼ {type(result[0])}")
            print(f"æƒ…æ„Ÿè¯†åˆ«ç»“æœ: {result}")
            return {
                "text": "",
                "emotion": "ğŸ˜",
                "events": "",
                "formatted_text": ""
            }
        
        # è·å–æ–‡æœ¬å¹¶å¤„ç†
        text = result[0]["text"]
        if text is None or not isinstance(text, str):
            print(f"æƒ…æ„Ÿè¯†åˆ«: æ–‡æœ¬ä¸æ˜¯å­—ç¬¦ä¸² {type(text)}")
            return {
                "text": "",
                "emotion": "ğŸ˜",
                "events": "",
                "formatted_text": ""
            }
        
        # å®‰å…¨åœ°æ ¼å¼åŒ–æƒ…æ„Ÿæ–‡æœ¬
        try:
            emotion_info = format_emotion_text(text)
            return emotion_info
        except Exception as e:
            print(f"æ ¼å¼åŒ–æƒ…æ„Ÿæ–‡æœ¬é”™è¯¯: {e}")
            return {
                "text": text,  # è‡³å°‘è¿”å›åŸå§‹æ–‡æœ¬
                "emotion": "ğŸ˜",
                "events": "",
                "formatted_text": text
            }
            
    except Exception as e:
        print(f"æƒ…æ„Ÿè¯†åˆ«é”™è¯¯: {str(e)}")
        return {
            "text": "",
            "emotion": "ğŸ˜",
            "events": "",
            "formatted_text": ""
        }
def load_model(model_key):
    """æ ¹æ®æ¨¡å‹é”®åŠ è½½ç›¸åº”æ¨¡å‹ï¼ŒåªåŠ è½½ASRæ¨¡å‹ï¼Œå…±ç”¨è¾…åŠ©æ¨¡å‹"""
    if model_key not in loaded_models:
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {models[model_key]['name']}...")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        model_path = models[model_key]['path']
        if not os.path.exists(model_path):
            print(f"è­¦å‘Š: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            print(f"è¯·ç¡®ä¿å·²ä¸‹è½½æ‰€æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶")
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        # åªä½¿ç”¨æœ¬åœ°æ¨¡å‹
        loaded_models[model_key] = AutoModel(
            model=model_path,
            disable_pbar=True,
            disable_log=True,
            disable_update=True,
            local_files_only=True,  # æ·»åŠ æ­¤å‚æ•°
            provider="local"         # æŒ‡å®šä½¿ç”¨æœ¬åœ°æä¾›è€…
        )
        print(f"æ¨¡å‹ {models[model_key]['name']} åŠ è½½å®Œæˆ")
    
    return loaded_models[model_key]
# åˆ›å»ºå…¨å±€æ—¶é—´æˆ³æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½
timestamp_model = None

# åœ¨æ¨¡å‹é…ç½®éƒ¨åˆ†æ·»åŠ 
emotion_model_path = "/home/chihan/workspace/SenseVoice/model_cache/models/iic/SenseVoiceSmall"  # SenseVoiceæ¨¡å‹ç”¨äºæƒ…æ„Ÿè¯†åˆ«
emotion_model_revision = "None"  # æœ¬åœ°æ¨¡å‹ä¸éœ€è¦revision

# æ·»åŠ è¡¨æƒ…æ˜ å°„å­—å…¸
emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "ğŸ˜",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
    "<|BGM|>": "ğŸµ",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜„",
    "<|Cry|>": "ğŸ˜­",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
}

# å®Œæ•´çš„emojiå­—å…¸
emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“",
    "<|zh|>": "",
    "<|en|>": "",
    "<|yue|>": "",
    "<|ja|>": "",
    "<|ko|>": "",
    "<|nospeech|>": "",
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "ğŸ˜",
    "<|BGM|>": "ğŸµ",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜„",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
    "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
    "<|Sing|>": "ğŸ¤",
    "<|Speech_Noise|>": "",
    "<|withitn|>": "",
    "<|woitn|>": "",
    "<|GBG|>": "",
    "<|Event_UNK|>": "",
}

# æƒ…æ„Ÿé›†åˆå’Œäº‹ä»¶é›†åˆï¼Œç”¨äºåå¤„ç†
emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸµ", "ğŸ‘", "ğŸ˜„", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·", "ğŸ¤"}
def initialize_app():
    """åˆå§‹åŒ–åº”ç”¨å¹¶åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
    global model, timestamp_model, emotion_model, cosyvoice_model
    
    # åŠ è½½CosyVoiceæ¨¡å‹ç”¨äºå£°éŸ³å…‹éš†
    print("æ­£åœ¨åŠ è½½CosyVoiceå£°éŸ³å…‹éš†æ¨¡å‹...")
    try:
        if cosyvoice_available:
            cosyvoice_model = CosyVoice2(
                '/home/chihan/workspace/SenseVoice/CosyVoice/pretrained_models/CosyVoice2-0.5B', 
                load_jit=False, 
                load_trt=False, 
                fp16=False
            )
            print("CosyVoiceå£°éŸ³å…‹éš†æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            cosyvoice_model = None
            print("CosyVoiceæ¨¡å—æœªå®‰è£…ï¼Œå£°éŸ³å…‹éš†åŠŸèƒ½ä¸å¯ç”¨")
    except Exception as e:
        cosyvoice_model = None
        print(f"CosyVoiceå£°éŸ³å…‹éš†æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("æ­£åœ¨åŠ è½½æ—¶é—´æˆ³å’Œè¯´è¯äººåˆ†ç¦»æ¨¡å‹...")
     # åŠ è½½æ—¶é—´æˆ³æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    timestamp_model = AutoModel(
    model="iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    model_revision="master",
    vad_model=vad_model_path,
    vad_model_revision=vad_model_revision,
    punc_model=punc_model_path,
    punc_model_revision=punc_model_revision,
    spk_model=spk_model_path,
    spk_model_revision=spk_model_revision,
    ngpu=ngpu,
    ncpu=ncpu,
    device=device,
    spk_threshold=0.2,       # é™ä½è¯´è¯äººåŒºåˆ†é˜ˆå€¼ï¼ˆé»˜è®¤æ˜¯0.5)
    disable_pbar=True,
    disable_log=True,
    disable_update=True,
    add_punc=True  # ç¡®ä¿æ­¤å‚æ•°è®¾ç½®ä¸ºTrue
)
    print("æ—¶é—´æˆ³å’Œè¯´è¯äººåˆ†ç¦»æ¨¡å‹åŠ è½½å®Œæˆ")
    # æ·»åŠ æ—¶é—´æˆ³æ¨¡å‹æµ‹è¯•ä»£ç 
    print("æ—¶é—´æˆ³æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æµ‹è¯•...")
    test_audio = b"\x00\x00" * 16000  # 1ç§’é™éŸ³
    test_timestamp_res = timestamp_model.generate(
        input=test_audio, 
        batch_size_s=60,
        batch_size=1,  # ç¡®ä¿åŒ…å«è¿™ä¸ªå‚æ•°
        is_final=True, 
        sentence_timestamp=True,
        speaker_change=True,
    add_punc=True  # ç¡®ä¿æ­¤å‚æ•°è®¾ç½®ä¸ºTrue
    )
    print(f"æ—¶é—´æˆ³æ¨¡å‹æµ‹è¯•ç»“æœ: {test_timestamp_res}")
    if test_timestamp_res and len(test_timestamp_res) > 0:
        print(f"ç»“æœåŒ…å«çš„é”®: {list(test_timestamp_res[0].keys())}")
        if 'text' not in test_timestamp_res[0]:
            print("è­¦å‘Šï¼šæ—¶é—´æˆ³æ¨¡å‹è¿”å›ç»“æœä¸­ç¼ºå°‘'text'å­—æ®µï¼")
    print("æ—¶é—´æˆ³å’Œè¯´è¯äººåˆ†ç¦»æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"æ­£åœ¨é¢„åŠ è½½é»˜è®¤æ¨¡å‹ {models[default_model]['name']}...")
    model = load_model(default_model)
    print(f"é»˜è®¤æ¨¡å‹ {models[default_model]['name']} åŠ è½½å®Œæˆ")
    
    print("æ­£åœ¨åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹...")
    # åŠ è½½SenseVoiceæƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼Œä½¿ç”¨å®Œå…¨æœ¬åœ°æ¨¡å¼
    emotion_model = AutoModel(
        model=emotion_model_path,
        model_revision=emotion_model_revision,
        vad_model=vad_model_path,
        vad_model_revision=vad_model_revision,
        ngpu=ngpu,
        ncpu=ncpu,
        device=device,
        disable_pbar=True,
        disable_log=True,
        disable_update=True,
        trust_remote_code=False,  # ä¿®æ”¹ä¸ºFalseï¼Œé¿å…æ‰§è¡Œè¿œç¨‹ä»£ç 
        local_files_only=True,    # å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        provider="local"          # æŒ‡å®šä½¿ç”¨æœ¬åœ°æä¾›è€…
    )
    
    # æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
    print("æƒ…æ„Ÿæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æµ‹è¯•...")
    
    # æµ‹è¯•æƒ…æ„Ÿæ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
    test_result = emotion_model.generate(
        input=b"\x00\x00" * 16000,  # 1ç§’é™éŸ³
        language="auto"
    )
    print(f"æƒ…æ„Ÿæ¨¡å‹æµ‹è¯•ç»“æœ: {test_result}")
    print("æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ")


# è¾…åŠ©å‡½æ•°
def to_date(milliseconds):
    """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºSRTæ ¼å¼çš„æ—¶é—´"""
    time_obj = timedelta(milliseconds=milliseconds)
    return f"{time_obj.seconds // 3600:02d}:{(time_obj.seconds // 60) % 60:02d}:{time_obj.seconds % 60:02d}.{time_obj.microseconds // 1000:03d}"

def to_milliseconds(time_str):
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¯«ç§’"""
    try:
        # ç¡®ä¿ä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²
        if not isinstance(time_str, str):
            print(f"è­¦å‘Š: éå­—ç¬¦ä¸²æ—¶é—´æ ¼å¼ {time_str}, ç±»å‹: {type(time_str)}")
            return 0
            
        time_obj = datetime.strptime(time_str, "%H:%M:%S.%f")
        time_delta = time_obj - datetime(1900, 1, 1)
        milliseconds = int(time_delta.total_seconds() * 1000)
        return milliseconds
    except Exception as e:
        print(f"æ—¶é—´è½¬æ¢é”™è¯¯: {e}, è¾“å…¥å€¼: {time_str}")
        return 0  # è¿”å›é»˜è®¤å€¼è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
def improve_text_formatting(text):
    """ä¸ºæ²¡æœ‰æ ‡ç‚¹çš„æ–‡æœ¬æ·»åŠ åŸºæœ¬æ ‡ç‚¹ç¬¦å·"""
    if not text:
        return ""
    
    # 1. æŒ‰è‡ªç„¶åœé¡¿æ·»åŠ é€—å·ï¼ˆé€šå¸¸æ˜¯5-10ä¸ªå­—ç¬¦ä¹‹åï¼‰
    chars = list(text)
    result = ""
    for i, char in enumerate(chars):
        result += char
        # æ¯8-12ä¸ªå­—ç¬¦æ£€æŸ¥æ˜¯å¦é€‚åˆæ·»åŠ é€—å·
        if i > 0 and (i+1) % 10 == 0 and i < len(chars)-1:
            # é¿å…åœ¨æŸäº›å­—ç¬¦åç›´æ¥åŠ é€—å·
            if char not in "ï¼Œã€‚ï¼Ÿï¼,.?!":
                result += "ï¼Œ"
    
    # 2. åœ¨å¥å­ç»“å°¾æ·»åŠ å¥å·
    if result and result[-1] not in "ï¼Œã€‚ï¼Ÿï¼,.?!":
        result += "ã€‚"
    
    return result
def clean_text(text):
    """ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šæ ‡è®°ä½†ä¿ç•™æ ‡ç‚¹ç¬¦å·"""
    import re
    # ç§»é™¤æ‰€æœ‰ç±»ä¼¼<|xyz|>æ ¼å¼çš„æ ‡è®°ï¼Œä½†ä¿ç•™æ ‡ç‚¹
    clean = re.sub(r'<\|[^|]*\|>', '', text)
    
    # ç§»é™¤å¯èƒ½çš„å¤šä½™ç©ºæ ¼ä½†ä¿ç•™æ ‡ç‚¹
    clean = re.sub(r'\s+', ' ', clean).strip()
    
    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«åŸºæœ¬æ ‡ç‚¹
    if not re.search(r'[ï¼Œã€‚ï¼Ÿï¼,.?!]', clean):
        # å¦‚æœæ²¡æœ‰æ ‡ç‚¹ï¼Œå¯ä»¥åœ¨æ­¤æ·»åŠ æ ‡ç‚¹æ¢å¤é€»è¾‘
        clean = improve_text_formatting(clean)
        
    return clean
# å¤„ç†éŸ³é¢‘åˆ†ç¦»
def process_audio(file_path, output_folder, model_key=default_model, split_number=10, enable_denoise=False, denoise_model="HP5", denoise_level=10):
    """
    å¤„ç†éŸ³é¢‘å¹¶è¯†åˆ«æ–‡æœ¬
    
    å‚æ•°:
    file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    output_folder: è¾“å‡ºç›®å½•
    model_key: ä½¿ç”¨çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹
    split_number: æ¯æ®µæ–‡å­—æ•°é‡
    enable_denoise: æ˜¯å¦å¯ç”¨é™å™ª
    denoise_model: é™å™ªæ¨¡å‹åç§°
    denoise_level: é™å™ªå¼ºåº¦çº§åˆ«(0-20)
    """
    try:
        if not os.path.exists(file_path):
            return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
        
        # ç¡®ä¿é€‰æ‹©çš„æ¨¡å‹æœ‰æ•ˆ
        if model_key not in models:
            return {"status": "error", "message": f"æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©: {model_key}"}
        
        audio_name = os.path.splitext(os.path.basename(file_path))[0]
        file_ext = os.path.splitext(file_path)[1].lower()
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if file_ext not in support_audio_format + support_video_format:
            return {"status": "error", "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}"}
        # ä¼˜å…ˆå¤„ç†é™å™ªï¼Œå¦‚æœå¯ç”¨äº†
        if enable_denoise and uvr_available:
            print(f"å¯¹éŸ³é¢‘è¿›è¡Œé™å™ªå¤„ç†ï¼Œä½¿ç”¨æ¨¡å‹: {denoise_model}, å¼ºåº¦: {denoise_level}")
            # åˆ›å»ºé™å™ªå¤„ç†åçš„æ–‡ä»¶è·¯å¾„
            denoised_file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"denoised_{int(time.time())}_{os.path.basename(file_path)}")
            
            # æ‰§è¡Œé™å™ªå¤„ç†
            denoised_result = process_denoise(file_path, denoise_model, int(denoise_level))
            
            if denoised_result and os.path.exists(denoised_result):
                print(f"é™å™ªæˆåŠŸï¼Œä½¿ç”¨é™å™ªåçš„æ–‡ä»¶è¿›è¡Œè¯†åˆ«: {denoised_result}")
                # ä½¿ç”¨é™å™ªåçš„æ–‡ä»¶
                file_path = denoised_result
            else:
                print(f"é™å™ªå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡ä»¶è¿›è¡Œè¯†åˆ«")
        try:
            # ä»æ–‡ä»¶ä¸­è¯»å–éŸ³é¢‘æ•°æ®
            audio_bytes, _ = (
                ffmpeg.input(file_path, threads=0)
                .output("-", format="wav", acodec="pcm_s16le", ac=1, ar=16000)
                .run(cmd=["ffmpeg", "-nostdin"], capture_stdout=True, capture_stderr=True)
            )
            
            # ä½¿ç”¨å…¨å±€æ—¶é—´æˆ³æ¨¡å‹è·å–è¯´è¯äººåˆ†ç¦»ä¿¡æ¯
             # ä½¿ç”¨å…¨å±€æ—¶é—´æˆ³æ¨¡å‹è·å–è¯´è¯äººåˆ†ç¦»ä¿¡æ¯
            timestamp_res = timestamp_model.generate(
    input=audio_bytes, 
    batch_size_s=300, 
    is_final=True, 
    sentence_timestamp=True,
    speaker_change=True,     # æ·»åŠ è¯´è¯äººå˜åŒ–æ£€æµ‹
    max_speaker_num=10,# è®¾ç½®æœ€å¤§è¯´è¯äººæ•°é‡
    add_punc=True  # ç¡®ä¿æ­¤å‚æ•°è®¾ç½®ä¸ºTrue       
)
            timestamp_result = timestamp_res[0]
            # æ·»åŠ è°ƒè¯•è¾“å‡º
            # æ‰“å°åŸå§‹ç»“æœä»¥ä¾¿è°ƒè¯•
            print(f"æ—¶é—´æˆ³æ¨¡å‹è¿”å›ç»“æœ: {timestamp_result}")
            print(f"ç»“æœåŒ…å«çš„é”®: {list(timestamp_result.keys())}")

            print(f"åŸå§‹è¯´è¯äººä¿¡æ¯: {[s['spk'] for s in timestamp_result['sentence_info']]}")
            print(f"ä¸åŒè¯´è¯äººæ•°é‡: {len(set([s['spk'] for s in timestamp_result['sentence_info']]))}")
            # æå–æ—¶é—´æˆ³ã€è¯´è¯äººå’Œæ–‡æœ¬ä¿¡æ¯
            asr_result_text = timestamp_result['text']
            sentences = []
            
            for sentence in timestamp_result["sentence_info"]:
                start = to_date(sentence["start"])
                end = to_date(sentence["end"])
                if sentences and sentence["spk"] == sentences[-1]["spk"] and len(sentences[-1]["text"]) < int(split_number):
                    sentences[-1]["text"] += "" + sentence["text"]
                    sentences[-1]["end"] = end
                else:
                    sentences.append(
                        {"text": sentence["text"], "start": start, "end": end, "spk": sentence["spk"]}
                    )
            
            # å¦‚æœéœ€è¦ï¼Œä½¿ç”¨é€‰å®šçš„ASRæ¨¡å‹æ›¿æ¢æ–‡æœ¬å†…å®¹ï¼ˆå¯é€‰ï¼‰
            if model_key != default_model and model_key != "sensevoice":
                try:
                    current_model = load_model(model_key)
                    # ç”¨æ–¹è¨€æ¨¡å‹çš„æ•´ä½“è¯†åˆ«ç»“æœæ›¿æ¢
                    text_res = current_model.generate(input=audio_bytes, batch_size_s=300,batch_size=1)
                    print(f"text_res: {text_res}.keys()={text_res[0].keys()}")
                    asr_result_text = text_res[0]['text']  # æ›¿æ¢æ•´ä½“æ–‡æœ¬
                except Exception as e:
                    print(f"æ¨¡å‹å¤„ç†å¼‚å¸¸: {e}")
                    # ç»§ç»­ä½¿ç”¨æ—¶é—´æˆ³æ¨¡å‹çš„æ–‡æœ¬ç»“æœ
            
            # åˆ›å»ºå¤„ç†ç»“æœç›®å½•
            date = datetime.now().strftime("%Y-%m-%d")
            output_path = os.path.join(output_folder, date, audio_name)
            os.makedirs(output_path, exist_ok=True)
            
            # å‰ªåˆ‡å’Œå¤„ç†æ¯ä¸ªå¥å­æ®µ
            speaker_audios = {}  # å­˜å‚¨æ¯ä¸ªè¯´è¯äººçš„éŸ³é¢‘ç‰‡æ®µ
            all_segments = []  # å­˜å‚¨æ‰€æœ‰åˆ†ç¦»çš„ç‰‡æ®µä¿¡æ¯
            i = 0
            for stn in sentences:
                stn_txt = stn['text']
                start = stn['start']
                end = stn['end']
                spk = stn['spk']
                
                # ä¸ºæ¯ä¸ªè¯´è¯äººåˆ›å»ºç›®å½•
                spk_path = os.path.join(output_path, str(spk))
                os.makedirs(spk_path, exist_ok=True)
                
                # ä¿å­˜éŸ³é¢‘ç‰‡æ®µ
                if file_ext in support_audio_format:
                    final_save_file = os.path.join(spk_path, f"{i}{file_ext}")
                    (
                        ffmpeg.input(file_path, threads=0, ss=start, to=end)
                        .output(final_save_file)
                        .run(cmd=["ffmpeg", "-nostdin"], overwrite_output=True, capture_stdout=True,
                             capture_stderr=True)
                    )
                elif file_ext in support_video_format:
                    final_save_file = os.path.join(spk_path, f"{i}.mp4")
                    (
                        ffmpeg.input(file_path, threads=0, ss=start, to=end)
                        .output(final_save_file, vcodec='libx264', crf=23, acodec='aac', ab='128k')
                        .run(cmd=["ffmpeg", "-nostdin"], overwrite_output=True, capture_stdout=True,
                             capture_stderr=True)
                    )
                
                # æ·»åŠ æƒ…æ„Ÿè¯†åˆ«éƒ¨åˆ† - åœ¨ä¸Šé¢ä»£ç åæ·»åŠ 
                # å¯¹åˆ†ç¦»çš„éŸ³é¢‘ç‰‡æ®µè¿›è¡Œæƒ…æ„Ÿè¯†åˆ«
                print(f"å¯¹ç‰‡æ®µ {i} è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«...")
                emotion_info = recognize_emotion(final_save_file)

                # ä¿å­˜æ–‡æœ¬ä¿¡æ¯
                spk_txt_file = os.path.join(output_path, f'spk{spk}.txt')
                with open(spk_txt_file, 'a', encoding='utf-8') as f:
                    # æ·»åŠ æƒ…æ„Ÿä¿¡æ¯åˆ°æ–‡æœ¬
                    f.write(f"{start} --> {end}\n{stn_txt} {emotion_info['emotion']}\n\n")

                # è®°å½•è¿™ä¸ªç‰‡æ®µçš„ä¿¡æ¯
                segment_info = {
                    'id': i,
                    'spk': spk,
                    'start': start or "00:00:00.000",  # ç¡®ä¿æœ‰é»˜è®¤å€¼
                    'end': end or "00:00:00.000",      # ç¡®ä¿æœ‰é»˜è®¤å€¼
                    'text': clean_text(stn_txt) if stn_txt else "",  # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºNone
                    'file': final_save_file,
                    # ä¿®æ”¹URLç”Ÿæˆæ–¹å¼ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
                    'url': f"/audio/{date}/{audio_name}/{spk}/{i}{file_ext if file_ext in support_audio_format else '.mp4'}",
                    # æ·»åŠ æƒ…æ„Ÿåˆ†æç»“æœ
                    'emotion': emotion_info['emotion'],
                    'events': emotion_info['events']
                }
                all_segments.append(segment_info)
                
                # è®°å½•è¯´è¯äººå’Œå¯¹åº”çš„éŸ³é¢‘ç‰‡æ®µ
                if spk not in speaker_audios:
                    speaker_audios[spk] = []
                speaker_audios[spk].append(segment_info)
                i += 1
            
            # æ³¨é‡Šæ‰æˆ–åˆ é™¤åˆå¹¶ä»£ç 
            # for spk, audio_segments in speaker_audios.items():
            #    output_file = os.path.join(output_path, f"{spk}.mp3")
            #    inputs = [seg['file'] for seg in audio_segments]
            #    concat_audio = AudioSegment.from_file(inputs[0])
            #    for i in range(1, len(inputs)):
            #        concat_audio = concat_audio + AudioSegment.from_file(inputs[i])
            #    concat_audio.export(output_file, format="mp3")
            
            # é‡æ–°è¯†åˆ«æ¯ä¸ªå¥å­å¹¶æ›´æ–°æ–‡æœ¬
            for i, stn in enumerate(sentences):
                # ä»åŸå§‹éŸ³é¢‘ä¸­æå–è¿™ä¸ªå¥å­çš„éŸ³é¢‘ç‰‡æ®µ
                segment_file = os.path.join(app.config['UPLOAD_FOLDER'], f"temp_{i}.wav")
                # æå–éŸ³é¢‘ç‰‡æ®µ
                (
                    ffmpeg.input(file_path, threads=0, ss=stn['start'], to=stn['end'])
                    .output(segment_file, acodec='pcm_s16le', ac=1, ar=16000)
                    .run(cmd=["ffmpeg", "-nostdin"], overwrite_output=True, capture_stdout=True,
                        capture_stderr=True)
                )
                
                # ä½¿ç”¨é€‰å®šæ¨¡å‹è¯†åˆ«å¹¶æ›´æ–°æ–‡æœ¬
                try:
                    current_model = load_model(model_key)
                    stn_res = current_model.generate(input=segment_file, batch_size_s=300,batch_size=1)
                    cleaned_text = clean_text(stn_res[0]['text'])
                    sentences[i]['text'] = cleaned_text
                    
                    # æ›´æ–°åˆ†æ®µæ–‡æœ¬
                    for segment in all_segments:
                        if segment['id'] == i and segment['spk'] == stn['spk']:
                            segment['text'] = cleaned_text
                            break
                    
                    print(f"ä½¿ç”¨ {models[model_key]['name']} æ¨¡å‹æˆåŠŸè¯†åˆ«åˆ†æ®µ {i}, è¯´è¯äºº {stn['spk']}")
                except Exception as e:
                    print(f"æ¨¡å‹å¤„ç†å¼‚å¸¸: {e}")
            
            # å…³é”®ä¿®æ”¹: é‡æ–°åˆæˆæ€»ä½“æ–‡æœ¬
            # åœ¨æ‰€æœ‰åˆ†æ®µéƒ½é‡æ–°è¯†åˆ«åï¼Œæ ¹æ®åˆ†æ®µé¡ºåºé‡ç»„æ€»ä½“æ–‡æœ¬
            combined_text = ""
            # æŒ‰æ—¶é—´é¡ºåºæ’åºæ‰€æœ‰ç‰‡æ®µ
            sorted_segments = sorted(all_segments, key=lambda x: x['start'] if x['start'] else "")
            
            # åˆå¹¶æ‰€æœ‰ç‰‡æ®µçš„æ–‡æœ¬
            for segment in sorted_segments:
                if segment['text']:
                    combined_text += segment['text'] + " "
            
            # å»é™¤å¤šä½™ç©ºæ ¼å¹¶æ¸…ç†
            asr_result_text = clean_text(combined_text)
            
            # è¿”å›æ›´æ–°åçš„ç»“æœ
            result = {
                "status": "success", 
                "message": f"ä½¿ç”¨ {models[model_key]['name']} æ¨¡å‹å¤„ç†å®Œæˆ",
                "text": asr_result_text,  # æ³¨æ„è¿™é‡Œä½¿ç”¨äº†æ›´æ–°åçš„æ–‡æœ¬
                "segments": all_segments or [],
                "speakers": list(speaker_audios.keys()) if speaker_audios else [],
                "model": models[model_key]['name'],
                "duration": to_milliseconds(sentences[-1]['end']) if sentences else 0
            }
            
            # å¦‚æœä½¿ç”¨äº†é™å™ªï¼Œæ·»åŠ åˆ°è¿”å›ç»“æœä¸­
            if enable_denoise:
                result["denoised"] = True
                result["denoise_model"] = denoise_model
                
            return result
            
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œè®°å½•å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
            error_message = str(e)
            print(f"éŸ³é¢‘å¤„ç†å¼‚å¸¸: {error_message}")
            return {"status": "error", "message": f"å¤„ç†å¤±è´¥: {error_message}"}
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for i in range(len(sentences) if 'sentences' in locals() else 0):
                temp_file = os.path.join(app.config['UPLOAD_FOLDER'], f"temp_{i}.wav")
                if os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except:
                        pass
    except Exception as e:
        print(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return jsonify({
            "status": "error", 
            "message": f"å¤„ç†å¤±è´¥: {str(e)}",
            "text": "",
            "segments": [],
            "speakers": [],
            "model": models[model_key]["name"],
            "duration": 0
        })

@app.route('/')
def index():
    return render_template('index.html')
@app.route('/audio/<path:filepath>')
def get_audio(filepath):
    """æä¾›å¯¹éŸ³é¢‘æ–‡ä»¶çš„è®¿é—®"""
    try:
        # æ‰“å°è·¯å¾„ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
        print(f"è®¿é—®éŸ³é¢‘: {filepath}")
        print(f"å®Œæ•´è·¯å¾„: {os.path.join(app.config['OUTPUT_FOLDER'], filepath)}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        full_path = os.path.join(app.config['OUTPUT_FOLDER'], filepath)
        if not os.path.exists(full_path):
            print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {full_path}")
            return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
            
        return send_from_directory(app.config['OUTPUT_FOLDER'], filepath)
    except Exception as e:
        print(f"éŸ³é¢‘è®¿é—®å¼‚å¸¸: {e}")
        return jsonify({"error": str(e)}), 500
# æ·»åŠ é™å™ªé¡µé¢è·¯ç”±
@app.route('/denoise_page')
def denoise_page():
    """è¿”å›é™å™ªå·¥å…·é¡µé¢"""
    return render_template('denoise.html')
@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({"status": "error", "message": "æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶"})
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"status": "error", "message": "æœªé€‰æ‹©æ–‡ä»¶"})
    
    # ä¿å­˜ä¸Šä¼ æ–‡ä»¶
    filename = werkzeug.utils.secure_filename(file.filename)
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    file.save(file_path)
    
    # è·å–åˆ†ç¦»å­—æ•°
    try:
        split_number = int(request.form.get('split_number', 10))
    except ValueError:
        split_number = 10  # é»˜è®¤å€¼
    
    # è·å–é€‰å®šçš„æ¨¡å‹
    model_key = request.form.get('model', default_model)
    if model_key not in models:
        model_key = default_model

    # è·å–é™å™ªå‚æ•°
    enable_denoise = request.form.get('enable_denoise') == 'true'
    denoise_model = request.form.get('denoise_model', 'HP5')
    try:
        denoise_level = int(request.form.get('denoise_level', 10))
    except ValueError:
        denoise_level = 10
    
    try:
        # å¤„ç†æ–‡ä»¶ï¼Œæ·»åŠ é™å™ªå‚æ•°
        result = process_audio(
            file_path, 
            app.config['OUTPUT_FOLDER'], 
            model_key, 
            split_number,
            enable_denoise,
            denoise_model,
            denoise_level
        )
        
        # ç¡®ä¿è¿”å›å€¼ä¸­çš„æ‰€æœ‰å­—æ®µéƒ½æœ‰æœ‰æ•ˆå€¼
        if "status" not in result:
            result["status"] = "error"
        if "message" not in result:
            result["message"] = "æœªçŸ¥é”™è¯¯"
        if "text" not in result or result["text"] is None:
            result["text"] = ""
        if "segments" not in result or result["segments"] is None:
            result["segments"] = []
        if "speakers" not in result or result["speakers"] is None:
            result["speakers"] = []
        if "model" not in result:
            result["model"] = models[model_key]["name"]
        if "duration" not in result:
            result["duration"] = 0
        
        return jsonify(result)
    except Exception as e:
        print(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return jsonify({
            "status": "error", 
            "message": f"å¤„ç†å¤±è´¥: {str(e)}",
            "text": "",
            "segments": [],
            "speakers": [],
            "model": models[model_key]["name"],
            "duration": 0
        })
@app.route('/clone', methods=['POST'])
def clone_voice():
    """ä½¿ç”¨CosyVoiceå…‹éš†è¯´è¯äººå£°éŸ³"""
    if not cosyvoice_available or cosyvoice_model is None:
        return jsonify({"status": "error", "message": "å£°éŸ³å…‹éš†åŠŸèƒ½ä¸å¯ç”¨ï¼ŒCosyVoiceæ¨¡å‹æœªåŠ è½½"})
    
    # è·å–è¯·æ±‚å‚æ•°
    data = request.json
    if not data:
        return jsonify({"status": "error", "message": "è¯·æ±‚æ ¼å¼é”™è¯¯ï¼Œéœ€è¦JSONæ•°æ®"})
    
    # å¿…éœ€å‚æ•°ï¼šè¯´è¯äººIDã€è¦åˆæˆçš„æ–‡æœ¬ã€æ—¥æœŸã€éŸ³é¢‘åç§°
    speaker_id = data.get('speaker_id')
    text = data.get('text')
    date = data.get('date')
    audio_name = data.get('audio_name')
    
    if not all([speaker_id, text, date, audio_name]):
        return jsonify({"status": "error", "message": "ç¼ºå°‘å¿…è¦å‚æ•°: speaker_id, text, date, audio_name"})
    
    try:
        # æ„å»ºè¯¥è¯´è¯äººéŸ³é¢‘æ–‡ä»¶å¤¹è·¯å¾„
        speaker_folder = os.path.join(app.config['OUTPUT_FOLDER'], date, audio_name, str(speaker_id))
        if not os.path.exists(speaker_folder):
            return jsonify({"status": "error", "message": f"æ‰¾ä¸åˆ°è¯´è¯äººéŸ³é¢‘æ–‡ä»¶å¤¹: {speaker_folder}"})
        
        # è·å–ç¬¬ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ä½œä¸ºå‚è€ƒå£°éŸ³
        audio_files = [f for f in os.listdir(speaker_folder) if f.endswith(tuple(support_audio_format))]
        if not audio_files:
            audio_files = [f for f in os.listdir(speaker_folder) if f.endswith('.mp4')]
        
        if not audio_files:
            return jsonify({"status": "error", "message": f"è¯´è¯äººæ–‡ä»¶å¤¹ä¸­æ²¡æœ‰éŸ³é¢‘æ–‡ä»¶: {speaker_folder}"})
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªéŸ³é¢‘ä½œä¸ºå‚è€ƒ
        prompt_path = os.path.join(speaker_folder, audio_files[0])
        print(f"ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶ä½œä¸ºå£°éŸ³å‚è€ƒ: {prompt_path}")
        
        # ä»æ–‡ä»¶åè·å–ç‰‡æ®µID
        segment_id = os.path.splitext(os.path.basename(prompt_path))[0]
        print(f"å‚è€ƒéŸ³é¢‘ç‰‡æ®µID: {segment_id}")
        
                # åœ¨å‡½æ•°clone_voiceä¸­ä¿®æ”¹å‚è€ƒæ–‡æœ¬è·å–é€»è¾‘
        spk_txt_file = os.path.join(app.config['OUTPUT_FOLDER'], date, audio_name, f'spk{speaker_id}.txt')
        reference_text = "è¿™æ˜¯ä¸€æ®µå‚è€ƒéŸ³é¢‘"  # é»˜è®¤æ–‡æœ¬ï¼Œä»…åœ¨è¯†åˆ«å¤±è´¥æ—¶ä½¿ç”¨

        # ç›´æ¥ä½¿ç”¨è¯­éŸ³è¯†åˆ«æ¨¡å‹è¯†åˆ«å‚è€ƒéŸ³é¢‘
        print(f"å¼€å§‹ç›´æ¥è¯†åˆ«å‚è€ƒéŸ³é¢‘: {prompt_path}")
        try:
            # å¤„ç†éŸ³é¢‘æ ¼å¼ï¼Œç¡®ä¿ä¸º16kå•å£°é“
            temp_wav = os.path.join(app.config['UPLOAD_FOLDER'], f"temp_rec_{uuid.uuid4()}.wav")
            (
                ffmpeg.input(prompt_path, threads=0)
                .output(temp_wav, format="wav", acodec="pcm_s16le", ac=1, ar=16000)
                .run(cmd=["ffmpeg", "-nostdin"], capture_stdout=True, capture_stderr=True)
            )
            
            # è¯»å–å¤„ç†åçš„éŸ³é¢‘
            with open(temp_wav, 'rb') as f:
                audio_bytes = f.read()
            
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹è¿›è¡Œè¯†åˆ«
            current_model = load_model(default_model)
            rec_result = current_model.generate(
                input=audio_bytes, 
                batch_size=1,
                batch_size_s=60
            )
            
            # ä»è¯†åˆ«ç»“æœä¸­æå–æ–‡æœ¬
            if rec_result and len(rec_result) > 0 and 'text' in rec_result[0]:
                recognized_text = rec_result[0]['text']
                reference_text = clean_text(recognized_text)
                print(f"å‚è€ƒéŸ³é¢‘è¯†åˆ«æˆåŠŸï¼Œæ–‡æœ¬: {reference_text}")
            else:
                print(f"å‚è€ƒéŸ³é¢‘è¯†åˆ«è¿”å›æ— æ•ˆç»“æœï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_wav):
                os.remove(temp_wav)
                
        except Exception as e:
            print(f"å‚è€ƒéŸ³é¢‘è¯†åˆ«å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨é»˜è®¤å‚è€ƒæ–‡æœ¬")
        # å¦‚æœæ˜¯mp4æ–‡ä»¶ï¼Œå…ˆæå–éŸ³é¢‘
        if prompt_path.endswith('.mp4'):
            temp_wav = os.path.join(app.config['UPLOAD_FOLDER'], f"temp_prompt_{uuid.uuid4()}.wav")
            (
                ffmpeg.input(prompt_path)
                .output(temp_wav, format="wav", acodec="pcm_s16le", ac=1, ar=16000)
                .run(cmd=["ffmpeg", "-nostdin"], capture_stdout=True, capture_stderr=True)
            )
            prompt_path = temp_wav
        
        
                
        # åŠ è½½promptéŸ³é¢‘
        prompt_speech_16k = load_wav(prompt_path, 16000)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        clone_folder = os.path.join(app.config['OUTPUT_FOLDER'], date, audio_name, 'cloned')
        os.makedirs(clone_folder, exist_ok=True)
        
        # ç”Ÿæˆå…‹éš†è¯­éŸ³
        output_filename = f"cloned_{speaker_id}_{int(time.time())}.wav"
        output_path = os.path.join(clone_folder, output_filename)
        
        # åœ¨æ‰¾åˆ°å‚è€ƒæ–‡æœ¬åï¼Œæ¸…ç†æ‰€æœ‰è¡¨æƒ…ç¬¦å·
        reference_text = clean_emoji_text(reference_text)
        print(f"æ¸…ç†è¡¨æƒ…åçš„å‚è€ƒæ–‡æœ¬: {reference_text}")

        # æ‰§è¡Œå£°éŸ³å…‹éš†æ—¶ä½¿ç”¨æ¸…ç†åçš„æ–‡æœ¬
        print(f"å¼€å§‹å£°éŸ³å…‹éš†ï¼Œå‚è€ƒæ–‡æœ¬: {reference_text}, åˆæˆæ–‡æœ¬: {text}")
        clone_generator = cosyvoice_model.inference_zero_shot(
            text,            # tts_text: è¦åˆæˆçš„æ–°æ–‡æœ¬
            reference_text,  # å·²æ¸…ç†è¡¨æƒ…çš„å‚è€ƒæ–‡æœ¬
            prompt_speech_16k,
            stream=False
        )
        
        # æ–¹æ³•1: ç›´æ¥éå†ç”Ÿæˆå™¨è·å–ç¬¬ä¸€ä¸ªç»“æœ
        for result in clone_generator:
            # åªå¤„ç†ç¬¬ä¸€ä¸ªç»“æœ
            torchaudio.save(
                output_path, 
                result['tts_speech'], 
                cosyvoice_model.sample_rate
            )
            
            # è¿”å›ç»“æœ
            return jsonify({
                "status": "success",
                "message": "å£°éŸ³å…‹éš†æˆåŠŸ",
                "audio_url": f"/audio/{date}/{audio_name}/cloned/{output_filename}",
                "speaker_id": speaker_id,
                "text": text
            })
        
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
        return jsonify({
            "status": "error",
            "message": "å£°éŸ³å…‹éš†ç”Ÿæˆå¤±è´¥ï¼Œæ²¡æœ‰ç»“æœè¿”å›"
        })
            
    except Exception as e:
        print(f"å£°éŸ³å…‹éš†å¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "status": "error", 
            "message": f"å£°éŸ³å…‹éš†å¤±è´¥: {str(e)}"
        })
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_wav' in locals() and os.path.exists(temp_wav):
            try:
                os.remove(temp_wav)
            except:
                pass
@app.route('/outputs/<path:filename>')
def download_file(filename):
    return send_from_directory(app.config['OUTPUT_FOLDER'], filename)

# æ·»åŠ æ¨¡å‹æŸ¥è¯¢API

@app.route('/models', methods=['GET'])
def get_models():
    """è¿”å›æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„åˆ—è¡¨"""
    model_list = []
    for key, info in models.items():
        if key != "timestamp":  # ä¸æ˜¾ç¤ºæ—¶é—´æˆ³æ¨¡å‹
            model_list.append({
                "id": key,
                "name": info["name"],
                "default": key == default_model
            })
    return jsonify({"models": model_list})

# åœ¨mainä¸­è°ƒç”¨
if __name__ == '__main__':
    print("SenseVoice è¯´è¯äººåˆ†ç¦»åº”ç”¨")
    initialize_app()  # åªåœ¨å®é™…è¿è¡Œæ—¶æ‰åˆå§‹åŒ–
    app.run(debug=False, port=5000)